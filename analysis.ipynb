{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before Analysis..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mne\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from decimal import Decimal, getcontext\n",
    "import pandas as pd\n",
    "import ast\n",
    "import openpyxl as op\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분석대상 이름 입력\n",
    "# 실행 전 경로 및 파일명 확인 필요\n",
    "\n",
    "name_raw = input(\"분석대상 이름을 입력하세요.\")\n",
    "\n",
    "# Loading saved TFR file\n",
    "file_path = rf'H:\\Mg_EEG\\tfr_files_gamma\\{name_raw}_7200_tfr.h5'\n",
    "tfr = mne.time_frequency.read_tfrs(file_path)\n",
    "print(f\"성공적으로 {name_raw}의 tfr 파일을 로딩하였습니다.\")\n",
    "\n",
    "# Saving raw file for back-up\n",
    "tfr_raw = tfr.copy()\n",
    "print(f\"{name_raw}의 raw 파일을 백업하였습니다.\")\n",
    "\n",
    "# plot 작성을 위한 TFR 평균 계산산\n",
    "def tfr_mean(tfr):\n",
    "    mean_electrode = np.mean(tfr.data, axis=0) # Averaging across electrode\n",
    "    mean_frequency = np.mean(mean_electrode, axis=0) # Averaging across frequency\n",
    "    return(mean_frequency)\n",
    "\n",
    "# name에서 알파벳만 제거하여 이름 복구\n",
    "name = re.sub(r'[a-zA-Z]', '', name_raw)\n",
    "print(name)\n",
    "print(f\"{name}의 TFR을 분석합니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw fif file 보고 싶다면 사용\n",
    "# clean_data.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터가 뭔가 이상할 때 리셋하는 복구 코드\n",
    "#tfr = tfr_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# large artifact data load\n",
    "csv_file_path = r'C:\\Users\\esin4\\OneDrive\\바탕 화면\\Github\\Mg_infusion_coma\\large_artifact.csv'\n",
    "\n",
    "large_artifact_data = pd.read_csv(csv_file_path, encoding='utf-8-sig')\n",
    "\n",
    "# 입력한 이름이 데이터프레임에 존재하는지 확인하고 좌표 가져오기\n",
    "if name in large_artifact_data['Name'].values:\n",
    "    # 입력한 이름에 해당하는 데이터 가져오기\n",
    "    coordinates = large_artifact_data[large_artifact_data['Name'] == name]['Coordinates'].values[0]\n",
    "    \n",
    "    # 좌표가 'skip'이면 빈 리스트로 설정\n",
    "    if coordinates == 'skip':\n",
    "        large_artifact = []\n",
    "    else:\n",
    "        # 좌표 문자열을 리스트로 변환\n",
    "        import ast\n",
    "        large_artifact = ast.literal_eval(coordinates)\n",
    "# 입력한 이름이 데이터프레임에 없으면 빈 리스트로 설정\n",
    "else:\n",
    "    print(f\"{name}이(가) 데이터에 없습니다.\")\n",
    "    large_artifact = []\n",
    "\n",
    "# 결과 출력\n",
    "print(\"large_artifact 리스트:\")\n",
    "print(large_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분석한 값 저장할 엑셀 파일\n",
    "wb = op.load_workbook(r\"C:\\Users\\esin4\\OneDrive\\바탕 화면\\Github\\Mg_infusion_coma\\Mg_infusion_data.xlsx\")\n",
    "row = int(input(\"데이터가 입력될 행을 입력하세요.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# independent t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before vs After segmentation\n",
    "tfr_before = tfr.copy().crop(tmin=0, tmax=3600, include_tmax=False)\n",
    "tfr_after = tfr.copy().crop(tmin=3600, tmax=7199, include_tmax=False)\n",
    "\n",
    "# Averaging\n",
    "before_mean = tfr_mean(tfr_before)\n",
    "after_mean = tfr_mean(tfr_after)\n",
    "\n",
    "# Indexing about Large artifact\n",
    "before_idx = []\n",
    "after_idx = []\n",
    "\n",
    "for t_start, t_end in large_artifact:\n",
    "    if t_start < 3600 and t_end < 3600:\n",
    "        before_idx.extend(range(t_start*200, t_end*200))\n",
    "    elif t_start < 3600 and t_end >= 3600:\n",
    "        before_idx.extend(range(t_start*200, 3600*200))\n",
    "        after_idx.extend(range(0, (t_end-3600)*200))\n",
    "    else:\n",
    "        after_idx.extend(range((t_start-3600)*200, (t_end-3600)*200))\n",
    "\n",
    "# Removing Large artifacts\n",
    "before_mean_clean = np.delete(before_mean, before_idx)\n",
    "after_mean_clean = np.delete(after_mean, after_idx)\n",
    "\n",
    "# independent t-test\n",
    "# precision\n",
    "getcontext().prec = 50\n",
    "\n",
    "print(f\"{name}, independent t test\")\n",
    "# mean of before vs after\n",
    "mean_before = np.mean(before_mean_clean)\n",
    "mean_after = np.mean(after_mean_clean)\n",
    "print(\"mean_before : \\n\", mean_before)\n",
    "print(\"mean_after : \\n\", mean_after)\n",
    "\n",
    "# t-test\n",
    "t_stat, p_value = stats.ttest_ind(before_mean_clean, after_mean_clean)\n",
    "\n",
    "# print to decimal object\n",
    "p_value_decimal = Decimal(p_value)\n",
    "print (\"t_statistics : \\n\", t_stat)\n",
    "p_value_str = f\"{p_value_decimal:.100f}\"\n",
    "p_value_num = float(p_value_str)\n",
    "print(f\"p-value: \\n {p_value_str}\")\n",
    "\n",
    "# Define Cohens'D \n",
    "def cohens_d(group1, group2):\n",
    "    # Calculate mean, sd\n",
    "    mean1, mean2 = np.mean(group1), np.mean(group2)\n",
    "    std1, std2 = np.std(group1, ddof=1), np.std(group2, ddof=1)\n",
    "    \n",
    "    # sample size\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    \n",
    "    # Calculate pooled sd\n",
    "    pooled_std = np.sqrt(((n1 - 1) * std1**2 + (n2 - 1) * std2**2) / (n1 + n2 - 2))\n",
    "    \n",
    "    # Cohen's d\n",
    "    d = (mean1 - mean2) / pooled_std\n",
    "    return d\n",
    "\n",
    "# Calculate Effect Size\n",
    "effect_size = cohens_d(before_mean, after_mean)\n",
    "print(\"effect size \\n\", effect_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Excel\n",
    "ws_i = wb[\"independent t\"]\n",
    "ws_i.cell(row=row, column=1).value = f\"{name}\"\n",
    "ws_i.cell(row=row, column=2).value = mean_before\n",
    "ws_i.cell(row=row, column=3).value = mean_after\n",
    "ws_i.cell(row=row, column=4).value = t_stat\n",
    "ws_i.cell(row=row, column=5).value = p_value\n",
    "ws_i.cell(row=row, column=7).value = effect_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paired t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before vs After segmentation\n",
    "tfr_before = tfr.copy().crop(tmin=0, tmax=3600, include_tmax=False)\n",
    "tfr_after = tfr.copy().crop(tmin=3600, tmax=7199, include_tmax=False)\n",
    "\n",
    "# Averaging\n",
    "before_mean = tfr_mean(tfr_before)\n",
    "after_mean = tfr_mean(tfr_after)\n",
    "\n",
    "# trim the data 무조건 7200초 맞춰서 이 부분 안 써도 되도록 해야함. \n",
    "if len(before_mean) > len(after_mean):\n",
    "    before_mean = before_mean[:len(after_mean)]\n",
    "elif len(before_mean) < len(after_mean):\n",
    "    after_mean = after_mean[:len(before_mean)]\n",
    "else:\n",
    "    quit\n",
    "\n",
    "len(before_mean) == len(after_mean)\n",
    "\n",
    "# Indexing about large artifact\n",
    "\n",
    "before_idx = set()\n",
    "after_idx = set()\n",
    "\n",
    "for t_start, t_end in large_artifact:\n",
    "    if t_start < 3600 and t_end < 3600:\n",
    "        before_idx.update(range(t_start*200, t_end*200))\n",
    "        after_idx.update(range(t_start*200, t_end*200))\n",
    "    elif t_start < 3600 and t_end >= 3600:\n",
    "        before_idx.update(range(t_start*200, 3600*200))\n",
    "        after_idx.update(range(t_start*200, 3600*200))\n",
    "        after_idx.update(range(0, (t_end-3600)*200))\n",
    "        before_idx.update(range(0, (t_end-3600)*200))\n",
    "    else:\n",
    "        after_idx.update(range((t_start-3600)*200, (t_end-3600)*200))\n",
    "        before_idx.update(range((t_start-3600)*200, (t_end-3600)*200))\n",
    "\n",
    "# Convert sets back to sorted lists\n",
    "before_idx = sorted(before_idx)\n",
    "after_idx = sorted(after_idx)\n",
    "\n",
    "before_mean_clean = np.delete(before_mean, before_idx)\n",
    "after_mean_clean = np.delete(after_mean, after_idx)\n",
    "\n",
    "# Paired t-test\n",
    "# precision\n",
    "getcontext().prec = 50\n",
    "\n",
    "print(f\"{name}, paired t test\")\n",
    "# mean of before vs after\n",
    "mean_before = np.mean(before_mean_clean)\n",
    "mean_after = np.mean(after_mean_clean)\n",
    "print(\"mean_before :\\n \", mean_before)\n",
    "print(\"mean_after : \\n\", mean_after)\n",
    "\n",
    "# Kolmogorov-Smirnov test\n",
    "difference = before_mean_clean-after_mean_clean\n",
    "ks_statistic, ks_p_value = stats.kstest(difference, stats.norm.cdf)\n",
    "print(f\"KS Statistic: \\n {ks_statistic}\")\n",
    "print(f\"KS p-value: \\n {ks_p_value}\")\n",
    "\n",
    "# t-test\n",
    "t_stat, p_value = stats.ttest_rel(before_mean, after_mean)\n",
    "\n",
    "# print to decimal object\n",
    "p_value_decimal = Decimal(p_value)\n",
    "print (\"t_statistics : \\n\", t_stat)\n",
    "p_value_str = f\"{p_value_decimal:.100f}\"\n",
    "print(f\"p-value: \\n {p_value_str}\")\n",
    "\n",
    "# Define Cohens'D \n",
    "def cohens_d(group1, group2):\n",
    "    # Calculate mean, sd\n",
    "    mean1, mean2 = np.mean(group1), np.mean(group2)\n",
    "    std1, std2 = np.std(group1, ddof=1), np.std(group2, ddof=1)\n",
    "    \n",
    "    # sample size\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    \n",
    "    # Calculate pooled sd\n",
    "    pooled_std = np.sqrt(((n1 - 1) * std1**2 + (n2 - 1) * std2**2) / (n1 + n2 - 2))\n",
    "    \n",
    "    # Cohen's d\n",
    "    d = (mean1 - mean2) / pooled_std\n",
    "    return d\n",
    "\n",
    "# Calculate Effect Size\n",
    "effect_size = cohens_d(before_mean, after_mean)\n",
    "print(\"effect size \\n\", effect_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Excel\n",
    "ws_p = wb[\"paired t\"]\n",
    "ws_p.cell(row=row, column=1).value = f\"{name}\"\n",
    "ws_p.cell(row=row, column=2).value = mean_before\n",
    "ws_p.cell(row=row, column=3).value = mean_after\n",
    "ws_p.cell(row=row, column=4).value = ks_statistic\n",
    "ws_p.cell(row=row, column=5).value = ks_p_value\n",
    "ws_p.cell(row=row, column=7).value = t_stat\n",
    "ws_p.cell(row=row, column=8).value = p_value\n",
    "ws_p.cell(row=row, column=10).value = effect_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis across Band"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paired t-test with Cohen's D test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주파수 대역별로 처리\n",
    "freq_ranges = {\n",
    "    'Delta': (1, 4),\n",
    "    'Theta': (4, 8),\n",
    "    'Alpha': (8, 13),\n",
    "    'Beta': (13, 30),\n",
    "    'gamma': (30, 90)\n",
    "}\n",
    "\n",
    "# Averaging by band and timing\n",
    "for band, (fmin, fmax) in freq_ranges.items():\n",
    "    tfr_band_before = tfr.copy().crop(tmin=0, tmax=3600, fmin=fmin, fmax=fmax, include_tmax=False)\n",
    "    tfr_band_after = tfr.copy().crop(tmin=3600, tmax=7199, fmin=fmin, fmax=fmax, include_tmax=False)\n",
    "\n",
    "    globals()[f'tfr_{band}_before'] = tfr_band_before\n",
    "    globals()[f'tfr_{band}_after'] = tfr_band_after\n",
    "\n",
    "    globals()[f'{band}_before_mean'] = tfr_mean(tfr_band_before)\n",
    "    globals()[f'{band}_after_mean'] = tfr_mean(tfr_band_after)\n",
    "\n",
    "# Make length same    \n",
    "for band in freq_ranges.keys():\n",
    "    before_mean = globals()[f'{band}_before_mean']\n",
    "    after_mean = globals()[f'{band}_after_mean']\n",
    "\n",
    "    if len(before_mean) > len(after_mean):\n",
    "        globals()[f'{band}_before_mean'] = before_mean[:len(after_mean)]\n",
    "    elif len(before_mean) < len(after_mean):\n",
    "        globals()[f'{band}_after_mean'] = after_mean[:len(before_mean)]\n",
    "    else:\n",
    "        print(f\"{band} 대역의 before와 after 길이가 이미 같습니다.\")\n",
    "\n",
    "    print(f\"{band} 대역 처리 완료: before 길이 = {len(globals()[f'{band}_before_mean'])}, after 길이 = {len(globals()[f'{band}_after_mean'])}\")\n",
    "\n",
    "# Removing large artifact by band\n",
    "def remove_artifacts(before_mean, after_mean, large_artifact):\n",
    "    before_idx = set()\n",
    "    after_idx = set()\n",
    "\n",
    "    for t_start, t_end in large_artifact:\n",
    "        if t_start < 3600 and t_end < 3600:\n",
    "            before_idx.update(range(t_start*200, t_end*200))\n",
    "            after_idx.update(range(t_start*200, t_end*200))\n",
    "        elif t_start < 3600 and t_end >= 3600:\n",
    "            before_idx.update(range(t_start*200, 3600*200))\n",
    "            after_idx.update(range(t_start*200, 3600*200))\n",
    "            after_idx.update(range(0, (t_end-3600)*200))\n",
    "            before_idx.update(range(0, (t_end-3600)*200))\n",
    "        else:\n",
    "            after_idx.update(range((t_start-3600)*200, (t_end-3600)*200))\n",
    "            before_idx.update(range((t_start-3600)*200, (t_end-3600)*200))\n",
    "\n",
    "    before_idx = sorted(before_idx)\n",
    "    after_idx = sorted(after_idx)\n",
    "\n",
    "    before_mean_clean = np.delete(before_mean, before_idx)\n",
    "    after_mean_clean = np.delete(after_mean, after_idx)\n",
    "\n",
    "    return before_mean_clean, after_mean_clean\n",
    "\n",
    "# 주파수 대역별로 artifact 제거\n",
    "for band in freq_ranges.keys():\n",
    "    before_mean = globals()[f'{band}_before_mean']\n",
    "    after_mean = globals()[f'{band}_after_mean']\n",
    "\n",
    "    before_mean_clean, after_mean_clean = remove_artifacts(before_mean, after_mean, large_artifact)\n",
    "\n",
    "    globals()[f'{band}_before_mean_clean'] = before_mean_clean\n",
    "    globals()[f'{band}_after_mean_clean'] = after_mean_clean\n",
    "\n",
    "    print(f\"{band} 대역 처리 완료:\")\n",
    "    print(f\"  Before: 원본 길이 = {len(before_mean)}, 정제 후 길이 = {len(before_mean_clean)}\")\n",
    "    print(f\"  After: 원본 길이 = {len(after_mean)}, 정제 후 길이 = {len(after_mean_clean)}\")\n",
    "    print(f\"  정제 후 Before와 After 길이 일치: {len(before_mean_clean) == len(after_mean_clean)}\")\n",
    "\n",
    "print(f\"\\n {name}, paired t test by band\")\n",
    "\n",
    "# 테스트 실행 함수\n",
    "def run_tests(before_mean, after_mean, band_name):\n",
    "    print(f\"\\n=== {band_name} 대역 분석 결과 ===\")\n",
    "    \n",
    "    # precision\n",
    "    getcontext().prec = 50\n",
    "\n",
    "    # mean of before vs after\n",
    "    mean_before = np.mean(before_mean)\n",
    "    mean_after = np.mean(after_mean)\n",
    "    print(f\"mean_before: \\n {mean_before}\")\n",
    "    print(f\"mean_after: \\n {mean_after}\")\n",
    "\n",
    "    # Kolmogorov-Smirnov test\n",
    "    difference = before_mean - after_mean\n",
    "    ks_statistic, ks_p_value = stats.kstest(difference, stats.norm.cdf)\n",
    "    print(f\"KS Statistic: \\n {ks_statistic}\")\n",
    "    print(f\"KS p-value: \\n {ks_p_value}\")\n",
    "\n",
    "    # t-test\n",
    "    t_stat, p_value = stats.ttest_rel(before_mean, after_mean)\n",
    "\n",
    "    # print to decimal object\n",
    "    p_value_decimal = Decimal(p_value)\n",
    "    print(f\"t_statistics: \\n {t_stat}\")\n",
    "    p_value_str = f\"{p_value_decimal:.100f}\"\n",
    "    print(f\"p-value: \\n {p_value_str}\")\n",
    "\n",
    "    # Define Cohens'D \n",
    "    def cohens_d(group1, group2):\n",
    "        mean1, mean2 = np.mean(group1), np.mean(group2)\n",
    "        std1, std2 = np.std(group1, ddof=1), np.std(group2, ddof=1)\n",
    "        n1, n2 = len(group1), len(group2)\n",
    "        pooled_std = np.sqrt(((n1 - 1) * std1**2 + (n2 - 1) * std2**2) / (n1 + n2 - 2))\n",
    "        d = (mean1 - mean2) / pooled_std\n",
    "        return d\n",
    "\n",
    "    # Calculate Effect Size\n",
    "    effect_size = cohens_d(before_mean, after_mean)\n",
    "    print(f\"Effect size: \\n {effect_size}\")\n",
    "\n",
    "    # 엑셀에 저장\n",
    "    ws_b = wb[\"Sheet1\"]\n",
    "    if band_name == 'Delta':\n",
    "        row_b = row*5-8\n",
    "    elif band_name == 'Theta':\n",
    "        row_b = row*5-7\n",
    "    elif band_name == 'Alpha':\n",
    "        row_b = row*5-6\n",
    "    elif band_name == 'Beta':\n",
    "        row_b = row*5-5\n",
    "    elif band_name == 'gamma':\n",
    "        row_b = row*5-4\n",
    "    else:\n",
    "        print(\"오류가 발생하였습니다.\")\n",
    "        return\n",
    "    \n",
    "    ws_b.cell(row=row_b, column=1).value = f\"{name}\"\n",
    "    ws_b.cell(row=row_b, column=4).value = mean_before\n",
    "    ws_b.cell(row=row_b, column=5).value = mean_after\n",
    "    ws_b.cell(row=row_b, column=6).value = ks_statistic\n",
    "    ws_b.cell(row=row_b, column=7).value = ks_p_value\n",
    "    ws_b.cell(row=row_b, column=9).value = t_stat\n",
    "    ws_b.cell(row=row_b, column=10).value = p_value\n",
    "    ws_b.cell(row=row_b, column=12).value = effect_size\n",
    "\n",
    "# 각 주파수 대역별로 테스트 실행\n",
    "for band in freq_ranges.keys():\n",
    "    before_mean = globals()[f'{band}_before_mean_clean']\n",
    "    after_mean = globals()[f'{band}_after_mean_clean']\n",
    "    run_tests(before_mean, after_mean, band)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DATA to Excel\n",
    "wb.save(r\"C:\\Users\\esin4\\OneDrive\\바탕 화면\\Github\\Mg_infusion_coma\\Mg_infusion_data.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실험적 접근"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the specific sheet into a DataFrame\n",
    "sheet_name = input(\"불러올 시트 이름을 입력하세요: \")\n",
    "df = pd.read_excel(r\"C:\\Users\\esin4\\OneDrive\\바탕 화면\\Github\\Mg_infusion_coma\\Mg_infusion_data.xlsx\", sheet_name=sheet_name)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre = df.loc[:, ['name', 'band', 'mean_before', 'mean_after']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre['Effect'] = df_pre['mean_before'] > df_pre['mean_after']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 중 효과있었던 밴드의 비율\n",
    "(df_pre.groupby('band')['Effect'].sum())/len(df_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot = df_pre.pivot(index='name', columns='band', values='Effect').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot = df_pivot[['name', 'delta', 'theta', 'alpha', 'beta', 'gamma']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_list = ['Y', 'Y-N', 'N', 'N', 'Y', 'Y-N', 'N', 'N', 'N', 'N', 'N', 'N', 'Y', 'N', 'N', 'Y', 'N', 'N', 'N', 'N', 'N', 'N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot['Eye'] = eye_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot['Eye_TF'] = (df_pivot['Eye'] == 'Y') | (df_pivot['Eye'] == 'Y-N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot.loc[df_pivot['Eye_TF']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "from scipy.stats import chi2_contingency\n",
    "import numpy as np\n",
    "\n",
    "def analyze_categorical_association(df, wave_band):\n",
    "    # 교차표 생성\n",
    "    contingency = pd.crosstab(df[wave_band], df['Eye_TF'])\n",
    "    \n",
    "    # 카이제곱 검정\n",
    "    chi2, p_value, dof, expected = chi2_contingency(contingency)\n",
    "    \n",
    "    # Cramer's V 계산\n",
    "    n = contingency.sum().sum()\n",
    "    min_dim = min(contingency.shape) - 1\n",
    "    cramer_v = np.sqrt(chi2 / (n * min_dim))\n",
    "    \n",
    "    return {\n",
    "        'wave_band': wave_band,\n",
    "        'chi2': chi2,\n",
    "        'p_value': p_value,\n",
    "        'cramer_v': cramer_v\n",
    "    }\n",
    "\n",
    "# 각 뇌파 대역에 대해 분석 수행\n",
    "results = []\n",
    "for band in ['delta', 'theta', 'alpha', 'beta', 'gamma']:\n",
    "    result = analyze_categorical_association(df_pivot, band)\n",
    "    results.append(result)\n",
    "\n",
    "# 결과를 데이터프레임으로 정리\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n연관성 분석 결과:\")\n",
    "print(results_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relative power 계산\n",
    "df_pre['mean_before_total'] = df_pre.groupby('name')['mean_before'].transform('sum')\n",
    "df_pre['mean_after_total'] = df_pre.groupby('name')['mean_after'].transform('sum')\n",
    "df_pre['relative_before'] = df_pre['mean_before'] / df_pre['mean_before_total']\n",
    "df_pre['relative_after'] = df_pre['mean_after'] / df_pre['mean_after_total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_list1 = []\n",
    "for i in eye_list:\n",
    "    eye_list1.append(i)\n",
    "    eye_list1.append(i)\n",
    "    eye_list1.append(i)\n",
    "    eye_list1.append(i)\n",
    "    eye_list1.append(i)\n",
    "eye_list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre['Eye_TF'] = eye_list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data_df):\n",
    "    \"\"\"\n",
    "    원본 데이터를 Repeated Measures ANOVA에 적합한 형태로 변환\n",
    "    \n",
    "    Parameters:\n",
    "    data_df: DataFrame with columns ['name', 'band', 'relative_before', 'relative_after']\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame with columns ['name', 'band', 'Condition', 'mean']\n",
    "    \"\"\"\n",
    "    # 데이터를 long format으로 변환\n",
    "    before_df = data_df[['name', 'band', 'mean_before', 'relative_before']].rename(\n",
    "        columns={'relative_before': 'relative_power', 'mean_before': 'mean_power'})\n",
    "    before_df['Condition'] = 'before'\n",
    "    \n",
    "    after_df = data_df[['name', 'band', 'mean_after', 'relative_after']].rename(\n",
    "        columns={'relative_after': 'relative_power', 'mean_after': 'mean_power'})\n",
    "    after_df['Condition'] = 'after'\n",
    "    \n",
    "    # 데이터 합치기\n",
    "    long_df = pd.concat([before_df, after_df], ignore_index=True)\n",
    "    \n",
    "    return long_df\n",
    "\n",
    "prepare_data(df_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "def prepare_data(data_df):\n",
    "    \"\"\"\n",
    "    원본 데이터를 Repeated Measures ANOVA에 적합한 형태로 변환\n",
    "    \n",
    "    Parameters:\n",
    "    data_df: DataFrame with columns ['name', 'band', 'relative_before', 'relative_after']\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame with columns ['name', 'band', 'Condition', 'mean']\n",
    "    \"\"\"\n",
    "    # 데이터를 long format으로 변환\n",
    "    before_df = data_df[['name', 'band', 'relative_before']].rename(\n",
    "        columns={'relative_before': 'relative_power'})\n",
    "    before_df['Condition'] = 'before'\n",
    "    \n",
    "    after_df = data_df[['name', 'band', 'relative_after']].rename(\n",
    "        columns={'relative_after': 'relative_power'})\n",
    "    after_df['Condition'] = 'after'\n",
    "    \n",
    "    # 데이터 합치기\n",
    "    long_df = pd.concat([before_df, after_df], ignore_index=True)\n",
    "    \n",
    "    return long_df\n",
    "\n",
    "def analyze_eeg_data(data_df):\n",
    "    \"\"\"\n",
    "    Two-way Repeated Measures ANOVA 분석 함수\n",
    "    \n",
    "    Parameters:\n",
    "    data_df: DataFrame with columns ['name', 'band', 'relative_before', 'relative_after']\n",
    "    \"\"\"\n",
    "    # 데이터 변환\n",
    "    long_data = prepare_data(data_df)\n",
    "    \n",
    "    # 1. Repeated Measures ANOVA 실행\n",
    "    rm_anova = AnovaRM(long_data, 'relative_power', 'name', within=['Condition', 'band'])\n",
    "    result = rm_anova.fit()\n",
    "    \n",
    "    # 2. 정규성 검정\n",
    "    conditions = ['before', 'after']\n",
    "    bands = data_df['band'].unique()\n",
    "    \n",
    "    print(\"Shapiro-Wilk Normality Test:\")\n",
    "    for cond in conditions:\n",
    "        for band in bands:\n",
    "            subset = long_data[(long_data['Condition'] == cond) & \n",
    "                             (long_data['band'] == band)]['relative_power']\n",
    "            stat, p = stats.shapiro(subset)\n",
    "            print(f\"{cond}-{band}: W={stat:.3f}, p={p:.3f}\")\n",
    "    \n",
    "    # 3-1. 사후 분석 (by T test)\n",
    "    def post_hoc_analysis_ttest(data_df):\n",
    "        post_hoc_results = []\n",
    "        for band in bands:\n",
    "            band_data = data_df[data_df['band'] == band]\n",
    "            t_stat, p_val = stats.ttest_rel(\n",
    "                band_data['relative_before'],\n",
    "                band_data['relative_after']\n",
    "            )\n",
    "            post_hoc_results.append({\n",
    "                'band': band,\n",
    "                't_statistic': t_stat,\n",
    "                'p_value': p_val\n",
    "            })\n",
    "        return pd.DataFrame(post_hoc_results)\n",
    "    \n",
    "\n",
    "    # 3-2. 사후 분석 (by Tukey HSD)\n",
    "    def post_hoc_analysis_tukey(data_df):\n",
    "        \"\"\"\n",
    "        Tukey HSD를 사용한 post-hoc 분석\n",
    "    \n",
    "        Parameters:\n",
    "        data_df: DataFrame with columns ['name', 'band', 'relative_before', 'relative_after']\n",
    "    \n",
    "        Returns:\n",
    "        Dictionary containing Tukey HSD results for each band\n",
    "        \"\"\"\n",
    "        # 데이터를 long format으로 변환\n",
    "        long_data = prepare_data(data_df)\n",
    "    \n",
    "        tukey_results = {}\n",
    "        for band in data_df['band'].unique():\n",
    "            # 해당 밴드의 데이터만 선택\n",
    "            band_data = long_data[long_data['band'] == band]\n",
    "        \n",
    "            # Tukey HSD 분석 수행\n",
    "            tukey = pairwise_tukeyhsd(\n",
    "                endog=band_data['relative_power'],\n",
    "                groups=band_data['Condition'],\n",
    "                alpha=0.05\n",
    "            )\n",
    "        \n",
    "            tukey_results[band] = tukey\n",
    "    \n",
    "        return tukey_results\n",
    "    \n",
    "    # 4. 시각화\n",
    "    def plot_results(long_data):\n",
    "        # Boxplot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.boxplot(x='band', y='relative_power', hue='Condition', data=long_data)\n",
    "        plt.title('Mean Values by Band and Condition')\n",
    "        plt.show()\n",
    "        \n",
    "        # Interaction plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        means = long_data.groupby(['band', 'Condition'])['relative_power'].mean().unstack()\n",
    "        means.plot(marker='o')\n",
    "        plt.title('Interaction Plot: Band x Condition')\n",
    "        plt.ylabel('Relative Power Value')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    \n",
    "    return {\n",
    "        'anova_results': result,\n",
    "        'post_hoc_ttest': post_hoc_analysis_ttest(data_df),\n",
    "        'post_hoc_tukey': post_hoc_analysis_tukey(data_df),\n",
    "        'plot_function': lambda: plot_results(long_data)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = analyze_eeg_data(df_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results['anova_results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results['post_hoc_ttest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for band, tukey_result in results['post_hoc_tukey'].items():\n",
    "    print(f\"Results for {band} band:\")\n",
    "    print(tukey_result.summary())\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 확인\n",
    "print(results['anova_results'])\n",
    "print(results['post_hoc_ttest'])\n",
    "results['plot_function']()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# 밴드 리스트\n",
    "bands = ['delta', 'theta', 'alpha', 'beta', 'gamma']\n",
    "\n",
    "data = df_pivot.copy()\n",
    "\n",
    "# 결과 저장\n",
    "results = []\n",
    "\n",
    "# 모든 조합 생성\n",
    "for r in range(1, len(bands) + 1):  # 조합 크기 1부터 모든 밴드까지\n",
    "    for combination in itertools.combinations(bands, r):\n",
    "        # 진단 기준: 조합에 포함된 모든 밴드가 True인 경우\n",
    "        data['diagnosis'] = data[list(combination)].all(axis=1)\n",
    "        \n",
    "        # 혼동 행렬 요소 계산\n",
    "        TP = ((data['diagnosis'] == True) & (data['Eye_TF'] == True)).sum()\n",
    "        TN = ((data['diagnosis'] == False) & (data['Eye_TF'] == False)).sum()\n",
    "        FP = ((data['diagnosis'] == True) & (data['Eye_TF'] == False)).sum()\n",
    "        FN = ((data['diagnosis'] == False) & (data['Eye_TF'] == True)).sum()\n",
    "        \n",
    "        # 민감도와 특이도 계산\n",
    "        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "        \n",
    "        # 결과 저장\n",
    "        results.append({\n",
    "            'combination': combination,\n",
    "            'sensitivity': sensitivity,\n",
    "            'specificity': specificity,\n",
    "            'TP': TP, 'TN': TN, 'FP': FP, 'FN': FN\n",
    "        })\n",
    "\n",
    "# 결과를 데이터프레임으로 정리\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# 결과 출력\n",
    "display(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
